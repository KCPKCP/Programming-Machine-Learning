{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fba3b9e",
   "metadata": {},
   "source": [
    "<h1> Keiland Pullen DSC 478 - Programming Machine Learning Homework #4 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ab734",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046aa56",
   "metadata": {},
   "source": [
    "<h1>Problem 1.</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705b885",
   "metadata": {},
   "source": [
    "<h2>Recommender System</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6e8c739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def ecludSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)\n",
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: continue\n",
    "        overLap = nonzero(logical_and(dataMat[:,item]>0, \\\n",
    "                                      dataMat[:,j]>0))[0]\n",
    "        if len(overLap) == 0: similarity = 0\n",
    "        else: similarity = simMeas(dataMat[overLap,item], \\\n",
    "                                   dataMat[overLap,j])\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "    \n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig4 = mat(eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:4] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T,\\\n",
    "                             xformedItems[j,:].T)\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "\n",
    "# This function is not needed for Assignment 4, but may be useful for experimentation\n",
    "def recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\n",
    "    unratedItems = nonzero(dataMat[user,:].A==0)[1] #find unrated items \n",
    "    if len(unratedItems) == 0: return 'you rated everything'\n",
    "    itemScores = []\n",
    "    for item in unratedItems:\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        itemScores.append((item, estimatedScore))\n",
    "    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]\n",
    "\n",
    "# This function performs evaluatoin on a single user based on the test_ratio\n",
    "# For example, with test_ratio = 0.2, a randomly selected 20 percent of rated \n",
    "# items by the user are withheld and the rest are used to estimate the withheld ratings\n",
    "\n",
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsSim):\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = int(test_ratio * len(rated_items_by_user))\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), test_size)\n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "    # Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + abs(estimatedScore - original_user_profile[item])\n",
    "\n",
    "    # Now restore ratings of the withheld items to the user profile\n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "\n",
    "    # Return sum of absolute errors and the count of test cases for this user\n",
    "    # Note that these will have to be accumulated for each user to compute MAE\n",
    "    return error_u, count_u\n",
    "    \n",
    "#def test(dataMat, test_ratio, estMethod):\n",
    "    # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "    # the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "    # across all test cases to the total number of test cases, for all users\n",
    "    #print ('Mean Absoloute Error for ',estMethod,' : ', MAE)\n",
    "\n",
    "#def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsSim):\n",
    "    # Write this function to find the k most similar jokes (based on user ratings) to a queryJoke\n",
    "    # The queryJoke is a joke id as given in the 'jokes.csv' file (an corresponding to the a column in dataMat)\n",
    "    # You must compare ratings for the queryJoke (the column in dataMat corresponding to the joke), to all\n",
    "    # other joke rating vectors and return the top k. Note that this is the same as performing KNN on the \n",
    "    # columns of dataMat. The function must retrieve the text of the joke from 'jokes.csv' file and print both\n",
    "    # the queryJoke text as well as the text of the returned jokes.\n",
    "        \n",
    "            \n",
    "def load_jokes(file):\n",
    "    jokes = genfromtxt(file, delimiter=',', dtype=str)\n",
    "    jokes = np.array(jokes[:,1])\n",
    "    return jokes\n",
    "\n",
    "def get_joke_text(jokes, id):\n",
    "    return jokes[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a140ecf",
   "metadata": {},
   "source": [
    "<h3>a.) Load in the joke ratings data and the joke text data into appropriate data structures.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9e102297",
   "metadata": {},
   "outputs": [],
   "source": [
    "jokeData = load_jokes('C:/Users/Home/Desktop/DePaul/Winter - DSC - 478 - Programming Machine Learning/Week 9/Homework-4/jokes/jokes.csv')\n",
    "\n",
    "jokeRatings = np.genfromtxt('C:/Users/Home/Desktop/DePaul/Winter - DSC - 478 - Programming Machine Learning/Week 9/Homework-4/jokes/modified_jester_data.csv', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "01162fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8a3ab9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokeRatings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d82647e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jokeRatings\n",
    "#jokeData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef998f69",
   "metadata": {},
   "source": [
    "<h3>b.) Complete the definition for the function \"test\".... </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e47e22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataMat, test_ratio, estMethod):\n",
    "    \n",
    "    #print ('Inside of test function')\n",
    "    #print ('The dataMat is', dataMat)\n",
    "    #print ('The test ratio is', test_ratio)\n",
    "    #print ('The estimation method is',estMethod)\n",
    "    \n",
    "    totalErr = 0\n",
    "    totalCnt = 0\n",
    "    \n",
    "    meanAbsErr = 0\n",
    "    \n",
    "    for user in range(1, dataMat.shape[1]):\n",
    "        data = cross_validate_user(dataMat, user, test_ratio, estMethod=standEst)\n",
    "        totalErr = totalErr + data[0]\n",
    "        totalCnt = totalCnt + data[1]\n",
    "        \n",
    "    meanAbsErr = totalErr/totalCnt\n",
    "    \n",
    "    \n",
    "    print('The Mean Absolute Error for ', estMethod,' is ',meanAbsErr )\n",
    "    \n",
    "    #print('End of Test function.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7549491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Error for  standEst  is  3.633388980526578\n"
     ]
    }
   ],
   "source": [
    "stand_Est = test(jokeRatings, 0.20, estMethod='standEst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "16187bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Error for  svdEst  is  3.6833326477202095\n"
     ]
    }
   ],
   "source": [
    "svd_Est = test(jokeRatings, 0.20, estMethod='svdEst')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff921778",
   "metadata": {},
   "source": [
    "<h3>c.) Write a new function \"print_most_similar_jokes\" which takes teh joke ratings data, ... </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "172e73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "850b21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsSim):\n",
    "    \n",
    "    dataMat_T = dataMat.T\n",
    "    user = dataMat_T.shape[0]\n",
    "    joke = dataMat_T[queryJoke]\n",
    "    \n",
    "    # print ('jokes :', jokes)\n",
    "    # print ('ratings :', dataMat)\n",
    "    \n",
    "    x = 0\n",
    "    similar_tot = np.zeros((user, 1))\n",
    "    similar_inx = np.zeros((user, 1))\n",
    "    \n",
    "    #print('Array of zeroes:', similar_tot)\n",
    "    #print('Array of zeroes:', similar_inx)\n",
    "    \n",
    "    while x < user:\n",
    "        similarity = metric(dataMat_T[x], joke)\n",
    "        similar_tot[x] = similarity\n",
    "        similar_inx[x] = x\n",
    "        x = x + 1\n",
    "    \n",
    "    #print(similar_tot)\n",
    "    #print(similar_inx)        \n",
    "   \n",
    "    #similar_inx = similar_inx.astype(int)\n",
    "    \n",
    "    AllJokes = np.concatenate( (similar_tot, similar_inx), axis = 1)\n",
    "    #print(AllJokes)\n",
    "    #print(type(AllJokes))\n",
    "    \n",
    "    print(\" The Query joke is, \", jokes[queryJoke])\n",
    "    print(\"\\n\")    \n",
    "    \n",
    "    similarJokes = []\n",
    "    \n",
    "    for i in AllJokes[:k]:\n",
    "        #print(AllJokes)\n",
    "        similarJokes.append(i)\n",
    "        \n",
    "    print(\" The Simnilar jokes are: \")\n",
    "    #print(similarJokes)\n",
    "    \n",
    "    z = 1\n",
    "    for jk in similarJokes:\n",
    "        #print(jokes[:])\n",
    "        #print(\"jk = \",jk[1:])\n",
    "        z = int(jk[1:])\n",
    "        print(z)\n",
    "        #print(type(jk))\n",
    "        print(jokes[z])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "1b9f6bd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Query joke is,  Two cannibals are eating a clown one turns to other and says: \"Does this taste funny to you?\n",
      "\n",
      "\n",
      " The Simnilar jokes are: \n",
      "0\n",
      "A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer's disease\". The man replies \"Well thank God I don't have cancer!\"\n",
      "1\n",
      "This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n",
      "2\n",
      "Q. What's 200 feet long and has 4 teeth? A. The front row at a Willie Nelson Concert.\n",
      "3\n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
      "4\n",
      "Q. What's O. J. Simpson's Internet address? A.\tSlash slash backslash slash slash escape.\n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(jokeRatings, jokeData, 9, 5,pearsSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bafb0b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Query joke is,  How many feminists does it take to screw in a light bulb?That's not funny.\n",
      "\n",
      "\n",
      " The Simnilar jokes are: \n",
      "0\n",
      "A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer's disease\". The man replies \"Well thank God I don't have cancer!\"\n",
      "1\n",
      "This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n",
      "2\n",
      "Q. What's 200 feet long and has 4 teeth? A. The front row at a Willie Nelson Concert.\n",
      "3\n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(jokeRatings, jokeData, 6, 4, ecludSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9c91bfd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15848/2713253688.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_most_similar_jokes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjokeRatings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjokeData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosSim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15848/3945835971.py\u001b[0m in \u001b[0;36mprint_most_similar_jokes\u001b[1;34m(dataMat, jokes, queryJoke, k, metric)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataMat_T\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoke\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0msimilar_tot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msimilar_inx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15848/3889459490.py\u001b[0m in \u001b[0;36mcosSim\u001b[1;34m(inA, inB)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcosSim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mla\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(jokeRatings, jokeData, 12, 4, metric = cosSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f44a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d66031c",
   "metadata": {},
   "source": [
    "<h1>Problem 2.)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee210705",
   "metadata": {},
   "source": [
    "<h2>PCA for Reduced Dimensionality in Clusters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e694c",
   "metadata": {},
   "source": [
    "<h3>a.) Load in the image data matrix....</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "48773337",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pd.read_table('C:/Users/Home/Desktop/DePaul/Winter - DSC - 478 - Programming Machine Learning/Week 9/Homework-4/segmentation_data/segmentation_classes.txt', names=['term', 'class'], header=None)\n",
    "features = pd.read_csv('C:/Users/Home/Desktop/DePaul/Winter - DSC - 478 - Programming Machine Learning/Week 9/Homework-4/segmentation_data/segmentation_data.txt', sep=',', header=None)\n",
    "names = pd.read_table('C:/Users/Home/Desktop/DePaul/Winter - DSC - 478 - Programming Machine Learning/Week 9/Homework-4/segmentation_data/segmentation_names.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8ef0e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "106c1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MinMax = MinMaxScaler()\n",
    "feat_norm = MinMax.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dc626",
   "metadata": {},
   "source": [
    "<h3>b.) Next, Perform Kmeans clustering...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "23bd8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "2786e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_clf = KMeans(n_clusters = 7)\n",
    "kmean_clf_fit = kmean_clf.fit(feat_norm)\n",
    "\n",
    "kmean_pred = kmean_clf_fit.predict(feat_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e35c4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmean_clf_fit.cluster_centers_\n",
    "#centers\n",
    "\n",
    "centers = centers.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9b49e09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.513994</td>\n",
       "      <td>0.770674</td>\n",
       "      <td>0.535099</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>0.254169</td>\n",
       "      <td>0.750696</td>\n",
       "      <td>0.302506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.808937</td>\n",
       "      <td>0.425215</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>0.393468</td>\n",
       "      <td>0.459974</td>\n",
       "      <td>0.534564</td>\n",
       "      <td>0.530862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.077441</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.074510</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.052260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.022581</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.046610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.054474</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.030228</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>0.037274</td>\n",
       "      <td>0.114419</td>\n",
       "      <td>0.100817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.019301</td>\n",
       "      <td>0.009420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046335</td>\n",
       "      <td>0.023122</td>\n",
       "      <td>0.026766</td>\n",
       "      <td>0.060574</td>\n",
       "      <td>0.027874</td>\n",
       "      <td>0.109240</td>\n",
       "      <td>0.083972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.011043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.108790</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.823246</td>\n",
       "      <td>0.148187</td>\n",
       "      <td>0.026013</td>\n",
       "      <td>0.300955</td>\n",
       "      <td>0.400608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.091403</td>\n",
       "      <td>0.035005</td>\n",
       "      <td>0.779716</td>\n",
       "      <td>0.138085</td>\n",
       "      <td>0.017925</td>\n",
       "      <td>0.279908</td>\n",
       "      <td>0.370347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.092414</td>\n",
       "      <td>0.058340</td>\n",
       "      <td>0.894170</td>\n",
       "      <td>0.185160</td>\n",
       "      <td>0.042306</td>\n",
       "      <td>0.352738</td>\n",
       "      <td>0.472461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.142676</td>\n",
       "      <td>0.028766</td>\n",
       "      <td>0.788761</td>\n",
       "      <td>0.118539</td>\n",
       "      <td>0.016504</td>\n",
       "      <td>0.265920</td>\n",
       "      <td>0.353036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.679161</td>\n",
       "      <td>0.778473</td>\n",
       "      <td>0.270665</td>\n",
       "      <td>0.716906</td>\n",
       "      <td>0.769800</td>\n",
       "      <td>0.592572</td>\n",
       "      <td>0.497146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.079002</td>\n",
       "      <td>0.223939</td>\n",
       "      <td>0.666373</td>\n",
       "      <td>0.344014</td>\n",
       "      <td>0.216011</td>\n",
       "      <td>0.451124</td>\n",
       "      <td>0.570882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.821287</td>\n",
       "      <td>0.486509</td>\n",
       "      <td>0.289386</td>\n",
       "      <td>0.355458</td>\n",
       "      <td>0.508117</td>\n",
       "      <td>0.309158</td>\n",
       "      <td>0.213054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.134901</td>\n",
       "      <td>0.059295</td>\n",
       "      <td>0.894170</td>\n",
       "      <td>0.185413</td>\n",
       "      <td>0.043295</td>\n",
       "      <td>0.352822</td>\n",
       "      <td>0.472461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.414491</td>\n",
       "      <td>0.538181</td>\n",
       "      <td>0.211804</td>\n",
       "      <td>0.411986</td>\n",
       "      <td>0.801687</td>\n",
       "      <td>0.302342</td>\n",
       "      <td>0.302263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.892333</td>\n",
       "      <td>0.243969</td>\n",
       "      <td>0.125066</td>\n",
       "      <td>0.201890</td>\n",
       "      <td>0.181153</td>\n",
       "      <td>0.164764</td>\n",
       "      <td>0.163879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cluster 1  Cluster 2  Cluster 3  Cluster 4  Cluster 5  Cluster 6  \\\n",
       "0    0.513994   0.770674   0.535099   0.256103   0.254169   0.750696   \n",
       "1    0.808937   0.425215   0.150167   0.393468   0.459974   0.534564   \n",
       "2    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3    0.077441   0.013978   0.027778   0.074510   0.026256   0.040000   \n",
       "4    0.005051   0.022581   0.001667   0.019118   0.013699   0.038462   \n",
       "5    0.054474   0.040237   0.030228   0.077343   0.037274   0.114419   \n",
       "6    0.001407   0.002989   0.000543   0.004100   0.002364   0.019301   \n",
       "7    0.046335   0.023122   0.026766   0.060574   0.027874   0.109240   \n",
       "8    0.001401   0.002088   0.000587   0.004967   0.002017   0.017999   \n",
       "9    0.108790   0.041138   0.823246   0.148187   0.026013   0.300955   \n",
       "10   0.091403   0.035005   0.779716   0.138085   0.017925   0.279908   \n",
       "11   0.092414   0.058340   0.894170   0.185160   0.042306   0.352738   \n",
       "12   0.142676   0.028766   0.788761   0.118539   0.016504   0.265920   \n",
       "13   0.679161   0.778473   0.270665   0.716906   0.769800   0.592572   \n",
       "14   0.079002   0.223939   0.666373   0.344014   0.216011   0.451124   \n",
       "15   0.821287   0.486509   0.289386   0.355458   0.508117   0.309158   \n",
       "16   0.134901   0.059295   0.894170   0.185413   0.043295   0.352822   \n",
       "17   0.414491   0.538181   0.211804   0.411986   0.801687   0.302342   \n",
       "18   0.892333   0.243969   0.125066   0.201890   0.181153   0.164764   \n",
       "\n",
       "    Cluster 7  \n",
       "0    0.302506  \n",
       "1    0.530862  \n",
       "2    0.000000  \n",
       "3    0.052260  \n",
       "4    0.046610  \n",
       "5    0.100817  \n",
       "6    0.009420  \n",
       "7    0.083972  \n",
       "8    0.011043  \n",
       "9    0.400608  \n",
       "10   0.370347  \n",
       "11   0.472461  \n",
       "12   0.353036  \n",
       "13   0.497146  \n",
       "14   0.570882  \n",
       "15   0.213054  \n",
       "16   0.472461  \n",
       "17   0.302263  \n",
       "18   0.163879  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(centers,columns=['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6', 'Cluster 7' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "5a3cef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity score is  0.6131870124853009\n",
      "Completeness score is  0.6115021163370862\n"
     ]
    }
   ],
   "source": [
    "score = homogeneity_completeness_v_measure(kmean_pred, classes['class'])\n",
    "\n",
    "print ('Homogeneity score is ', score[0] )\n",
    "print ('Completeness score is ', score[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0f968",
   "metadata": {},
   "source": [
    "<h3>c.) Perform PCA on the normalized image data matrix...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "bb690e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4c03916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca variance ration:  [0.60714234 0.13196979 0.10123773 0.04543539 0.03547361]\n",
      "total explained variance:  0.9212588648109568\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "\n",
    "pca_fit = pca.fit(feat_norm)\n",
    "\n",
    "print('pca variance ration: ',pca.explained_variance_ratio_)\n",
    "print('total explained variance: ', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "465496e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca variance ration:  [0.60714234 0.13196979 0.10123773 0.04543539 0.03547361 0.01988035]\n",
      "total explained variance:  0.9411392197960617\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 6)\n",
    "\n",
    "pca_fit = pca.fit(feat_norm)\n",
    "\n",
    "print('pca variance ration: ',pca.explained_variance_ratio_)\n",
    "print('total explained variance: ', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d7b232bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca variance ration:  [0.60714234 0.13196979 0.10123773 0.04543539 0.03547361 0.01988035\n",
      " 0.0189197 ]\n",
      "total explained variance:  0.9600589227704962\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 7)\n",
    "\n",
    "pca_fit = pca.fit(feat_norm)\n",
    "\n",
    "print('pca variance ration: ',pca.explained_variance_ratio_)\n",
    "print('total explained variance: ', pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "55e95b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 7)\n",
    "pca_fit = pca.fit(feat_norm)\n",
    "\n",
    "transformed_pca_fit = pca.fit_transform(feat_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122e5a02",
   "metadata": {},
   "source": [
    "<h3>d.) Perform Kmeans again, but this time on the lower dimensional transformed data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "8f61e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_clf_2 = KMeans(n_clusters = 7)\n",
    "kmean_clf_fit_2 = kmean_clf.fit(transformed_pca_fit)\n",
    "\n",
    "kmean_pred_2 = kmean_clf_fit.predict(transformed_pca_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "64148a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_2 = kmean_clf_fit_2.cluster_centers_\n",
    "#centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "583c3f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.176404</td>\n",
       "      <td>0.043698</td>\n",
       "      <td>-0.265370</td>\n",
       "      <td>0.184124</td>\n",
       "      <td>0.027075</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.003264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.414527</td>\n",
       "      <td>0.087223</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>-0.173195</td>\n",
       "      <td>-0.029922</td>\n",
       "      <td>-0.008973</td>\n",
       "      <td>-0.021573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.511579</td>\n",
       "      <td>-0.064908</td>\n",
       "      <td>-0.336145</td>\n",
       "      <td>-0.065365</td>\n",
       "      <td>0.078809</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>-0.026261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.619268</td>\n",
       "      <td>0.640250</td>\n",
       "      <td>0.195829</td>\n",
       "      <td>-0.086856</td>\n",
       "      <td>-0.067760</td>\n",
       "      <td>0.008866</td>\n",
       "      <td>0.038372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.206229</td>\n",
       "      <td>-0.246205</td>\n",
       "      <td>0.152785</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>-0.005540</td>\n",
       "      <td>0.032849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.603705</td>\n",
       "      <td>-0.355503</td>\n",
       "      <td>0.109197</td>\n",
       "      <td>-0.129799</td>\n",
       "      <td>-0.130911</td>\n",
       "      <td>-0.021603</td>\n",
       "      <td>-0.043882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.436890</td>\n",
       "      <td>-0.104936</td>\n",
       "      <td>0.164951</td>\n",
       "      <td>0.234379</td>\n",
       "      <td>-0.046159</td>\n",
       "      <td>-0.007512</td>\n",
       "      <td>0.015253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cluster 1  Cluster 2  Cluster 3  Cluster 4  Cluster 5  Cluster 6  Cluster 7\n",
       "0   0.176404   0.043698  -0.265370   0.184124   0.027075   0.024341   0.003264\n",
       "1   1.414527   0.087223   0.036765  -0.173195  -0.029922  -0.008973  -0.021573\n",
       "2  -0.511579  -0.064908  -0.336145  -0.065365   0.078809   0.006240  -0.026261\n",
       "3  -0.619268   0.640250   0.195829  -0.086856  -0.067760   0.008866   0.038372\n",
       "4  -0.206229  -0.246205   0.152785   0.056522   0.130585  -0.005540   0.032849\n",
       "5  -0.603705  -0.355503   0.109197  -0.129799  -0.130911  -0.021603  -0.043882\n",
       "6   0.436890  -0.104936   0.164951   0.234379  -0.046159  -0.007512   0.015253"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(centers_2,columns=['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5', 'Cluster 6', 'Cluster 7' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1cb851bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity score is  0.6107955063694607\n",
      "Completeness score is  0.6091364049733291\n"
     ]
    }
   ],
   "source": [
    "score = homogeneity_completeness_v_measure(kmean_pred_2, classes['class'])\n",
    "\n",
    "print ('Homogeneity score is ', score[0] )\n",
    "print ('Completeness score is ', score[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917151a",
   "metadata": {},
   "source": [
    "<h3>e.) Discuss your oberservations based on the comparison of the two clustering results.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6050a",
   "metadata": {},
   "source": [
    "The clustering results appear to be very similar.  However, using the transformed data for dimensionality reduction with n = 7 (which captures 95% of the variance) features would be optimal instead of using all of the features, as this could improve performance time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
